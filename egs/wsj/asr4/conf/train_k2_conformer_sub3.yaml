# loss_func
loss: k2
output_beam: 20
# batch
batch_size: 2
# data pipeline
num_workers: 4
# multi-gpu training
strategy: ddp
# optimiser
#optimiser: adam
#learning_rate: 1e-3
#factor: 0.2
#eps: 1e-8
#weight_decay: 0.
#lr_patience: 2
# random seed
seed: 777
# early_stopping
early_stopping: true
patience: 5
# for debugging
max_steps: -1
max_epochs: 1000

# model

accumulate_grad_batches: 8
idim: 83
adim: 512
aheads: 8
dlayers: 6
dropout-rate: 0.1
dunits: 2048
elayers: 12
eunits: 2048
grad-clip: 5
transformer-attn-dropout-rate: 0.0
transformer-input-layer: conv2d3
transformer-length-normalized-loss: false
transformer-lr: 1.0
transformer-warmup-steps: 25000
transformer-encoder-activation-type: swish
transformer-encoder-pos-enc-layer-type: rel_pos
transformer-encoder-selfattn-layer-type: rel_selfattn
macaron-style: true
use-cnn-module: true
cnn-module-kernel: 31

# SpecAugument
spec_aug: true
mode: "PIL"
max_time_warp: 5
max_freq_width: 30
n_freq_mask: 2
max_time_width: 40
n_time_mask: 2
inplace: true
replace_with_zero: false
