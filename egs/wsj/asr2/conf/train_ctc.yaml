# loss_func
loss: ctc
# batch
batch_size: 6
# data pipeline
num_workers: 12
# multi-gpu training
strategy: ddp
# optimiser
#optimiser: adam
#learning_rate: 1e-3
#factor: 0.2
#eps: 1e-8
#weight_decay: 0.
#lr_patience: 2
# random seed
seed: 777
# early_stopping
early_stopping: true
patience: 4
# for debugging
max_steps: -1
max_epochs: 1000

# fine-tuning

model: WAV2VEC2_LARGE_LV60K
finetune_layers: 2
