#!/usr/bin/env python3

import k2
import kaldiio
import os
import sys
import logging
import torch
import openfst_python as fst
import numpy as np
import argparse

"""
This script is used to analyze the lattice.
To run this script, you need to first do the decoding and get the decoding results
in a directory, e.g., outputs/*/*/decode_test/acwt_1.0-maxac_2000-beam-32/split10,
where in this directory, you can find the hyp.*.wrd which has the format
utterance-id wid1 wid2 wid3 ...
Besides, in outputs/*/*/split*, there should be a file named output.1.scp, from which 
this script load the log probabilities, which are the output of the neural network.
All the above file should exist after you do a full and successful decoding.

We also need to prepare a decoding graph TLG.pt, which can be compatible with k2.
This graph can be generated by the script utils/compile_TLG.py.
You can have a look at the script utils/compile_TLG.py to see how to generate this graph.

This script will load the decoding results and the log probabilities, and then
generate the lattice for each utterance. Then, it will compare the lattice with the
decoding results and outputs three files:
    1. WSumInLVsSumL.SearchBeam%d.LatticeBeam%d.${nj}.txt: utterance-id float, which is the P^S_L(W) / P^S_L
    2. WSumInLVsWSumInTLG.SearchBeam%d.LatticeBeam%d.${nj}.txt: utterance-id float, which is the P^S_L(W) / P^S_D(W)
    3. WBestInWSumInV.SearchBeam%d.LatticeBeam%d.${nj}.txt: utterance-id float, which is the P^B_L(W) / P^S_L(W)

Here are some explanations:
    1. P^S_L(W) is the sum of the probabilities of all the paths corresponding to the best word sequence W in the lattice.
    2. P^S_L is the sum of the probabilities of all the paths in the lattice.
    3. P^S_D(W) is the sum of the probabilities of all the paths corresponding to the best word sequence W in the decoding graph.
    4. P^B_L(W) is the best path probability corresponding to the best word sequence W in the lattice.
"""

def virtual_lattice_decoder(lattice, one_best_aux_label):
    """
    Args:
    lattice: The lattice generated by k2.
    one_best_aux_label: The aux_labels of the one-best path in the lattice.

    Returns:
    det_lattice: The deterministic lattice generated by the virtual lattice decoder.
    det_one_best_aux_label: The aux_labels of the one-best path in the deterministic lattice.
    """
    compiler = fst.Compiler(arc_type='log', keep_isymbols=False, keep_osymbols=False, acceptor=False)
    compiler.write(k2.to_str_simple(lattice, openfst=True))
    lattice_fst = compiler.compile()

    lattice_fst.project(project_output=True)
    lattice_fst = fst.determinize(lattice_fst)
    lattice_fst.rmepsilon()
    lattice_fst = fst.determinize(lattice_fst)
    lattice_fst.minimize()

    lattice_fst_str = lattice_fst.__str__()

    assert lattice_fst.verify(), 'The lattice is not valid.'

    det_lattice_fst = k2.Fsa.from_openfst(lattice_fst_str, acceptor=False)
    det_lattice_fst = k2.create_fsa_vec([det_lattice_fst])
    det_lattice_fst = k2.top_sort(det_lattice_fst)

    logging.info(f'lattice_deter.get_tot_scores(True, True) log semiring {det_lattice_fst.get_tot_scores(True, True)}')
    logging.info(f'lattice_deter.get_tot_scores(True, False) tropical semiring {det_lattice_fst.get_tot_scores(True, False)}')

    det_one_best = k2.shortest_path(det_lattice_fst, True)
    det_one_best_aux_label = k2.get_aux_labels(det_one_best)

    logging.info(f'det_one_best_aux_label {det_one_best_aux_label}')
    if not (det_one_best_aux_label == one_best_aux_label):
        logging.info('det_one_best_aux_label != one_best_aux_label')
        logging.info(f'det_one_best_aux_label {det_one_best_aux_label}')
        logging.info(f'one_best_aux_label {one_best_aux_label}')
    else:
        logging.info('det_one_best_aux_label == one_best_aux_label')
    logging.info(f'det_one_best.get_tot_scores(True, True) {det_one_best.get_tot_scores(True, True)}')
    logging.info(f'det_one_best.get_tot_scores(True, False) {det_one_best.get_tot_scores(True, False)}')




def analyse_one(TLG, 
                log_prob, 
                uttid, 
                acoustic_scale, 
                search_beam, 
                lattice_beam):
    """
    Args:
    TLG: The decoding graph, which is an FsaVec in k2.
    log_prob: The log probabilities of the decoding results.
    uttid: The utterance id.
    acoustic_scale: The acoustic scale used in decoding.
    search_beam: The search beam used in decoding.
    lattice_beam: The lattice beam used in decoding.

    Returns:
    Three float numbers, which are the P^S_L(W) / P^S_L, P^S_L(W) / P^S_D(W), P^B_L(W) / P^S_L(W).
    """
    logging.info(f'Analysing {uttid}')

    log_prob_len = torch.tensor([log_prob.shape[0]])
    log_prob = torch.tensor(log_prob).unsqueeze(0) * acoustic_scale

    supervision_segments = torch.tensor([[0, 0, log_prob.shape[1]]], device=log_prob.device, dtype=torch.int32)
    dense_fsa_vec = k2.DenseFsaVec(log_prob, supervision_segments)
    lattice = k2.get_lattice(log_prob, log_prob_len, TLG, search_beam=search_beam, output_beam=lattice_beam)
    lattice = k2.invert(k2.invert(lattice)) # invert the lattice twice to make its aux_labels back to torch.Tensor

    lattice_total_score = lattice.get_tot_scores(True, True)
    lattice_best_score = lattice.get_tot_scores(True, False)
    logging.info(f'lattice.get_tot_scores(True, True), log semiring {lattice_total_score}')
    logging.info('lattice.get_tot_scores(True, False), tropical semiring {lattice_best_score}')

    one_best = k2.one_best_decoding(lattice)
    one_best_aux_label = k2.get_aux_labels(one_best)
    logging.info(f'one_best.labels {one_best.labels}')
    logging.info(f'one_best_aux_label {one_best_aux_label}')
    W = k2.linear_fst(one_best_aux_label, one_best_aux_label)
    W = k2.arc_sort(W)
    latticeW = k2.connect(k2.compose(lattice, W))
    TLGW = k2.connect(k2.compose(TLG, W))

    ETLGW = k2.intersect_dense(TLGW, dense_fsa_vec, 10)
    TLG_W_total_score = ETLGW.get_tot_scores(True, True)
    TLG_W_best_score = ETLGW.get_tot_scores(True, False)
    logging.info(f'ETLGW.get_total_scores(True, True), log semiring, {TLG_W_total_score}')
    logging.info(f'ETLGW.get_total_scores(True, False), tropical semiring, {TLG_W_best_score}')

    lattice_W_total_score = latticeW.get_tot_scores(True, True)
    lattice_W_best_score = latticeW.get_tot_scores(True, False)
    logging.info(f'latticeW.get_tot_scores(True, True), log semiring, {lattice_W_total_score}')
    logging.info(f'latticeW.get_tot_scores(True, False), tropical semiring, {lattice_W_best_score}')


    logging.info(f'one_best.total_scores(True, True), log semiring {one_best.get_tot_scores(True, True)}')
    logging.info(f'one_best.total_scores(True, False), tropical semiring {one_best.get_tot_scores(True, False)}')

    WSumInLVsSumL = np.exp(lattice_W_total_score - lattice_total_score).item()
    WSumInLVsWSumInTLG = np.exp(lattice_W_total_score - TLG_W_total_score).item()
    WBestInLVsWSumInLV = np.exp(lattice_W_best_score - lattice_W_total_score).item()
    logging.info(f'W proportion in Lattice, {WSumInLVsSumL}')
    logging.info(f"W in Lattice / W in TLG, {WSumInLVsWSumInTLG}")
    logging.info(f"W best in Lattice / W sum in Lattice, {WBestInLVsWSumInLV}")

    if WSumInLVsSumL < 0.5:
        logging.info(f'WSumInLVsSumL < 0.5, {uttid}')
        virtual_lattice_decoder(lattice[0], one_best_aux_label)

    return WSumInLVsSumL, WSumInLVsWSumInTLG, WBestInLVsWSumInLV


def main():
    parser = argparse.ArgumentParser(description='Analyse the decoding results')
    parser.add_argument('--TLG', type=str, help='The decoding graph, e.g, data/lang/TLG.pt')
    parser.add_argument('--scp-split-dir', type=str, help='The log probabilities of the decoding results, e.g., outputs/*/*/decode_test/split*')
    parser.add_argument('--acoustic-scale', type=float, help='The acoustic scale used to get the lattice')
    parser.add_argument('--search-beam', type=int, help='The search beam used in decoding')
    parser.add_argument('--lattice-beam', type=int, help='The lattice beam used in decoding')
    parser.add_argument('--nj', type=int, help='The job number')
    args = parser.parse_args()

    TLG= k2.Fsa.from_dict(torch.load(args.TLG))
    TLG= k2.invert(k2.invert(TLG)) # invent the TLG twice to make its aux_labels back to torch.Tensor


    output_scp = os.path.join(args.scp_split_dir, 'output.%d.scp' % (args.nj))

    f1 = open(os.path.join(args.scp_split_dir, 'WSumInL.SumL.SearchBeam%d.LatticeBeam%d.%d.txt' % (args.search_beam, args.lattice_beam, args.nj)), 'w')
    f1c = ''
    f2 = open(os.path.join(args.scp_split_dir, 'WSumInL.WSumInTLG.SearchBeam%d.LatticeBeam%d.%d.txt' % (args.search_beam, args.lattice_beam, args.nj)), 'w')
    f2c = ''
    f3 = open(os.path.join(args.scp_split_dir, 'WBestInL.WSumInL.SearchBeam%d.LatticeBeam%d.%d.txt' % (args.search_beam, args.lattice_beam, args.nj)), 'w')
    f3c = ''


    with kaldiio.ReadHelper('scp:%s' % (output_scp)) as reader:
        for uttid, log_prob in reader:
            p1, p2, p3 = analyse_one(TLG, log_prob, uttid, args.acoustic_scale, args.search_beam, args.lattice_beam)
            f1c += '%s %.10f\n' % (uttid, p1)
            f2c += '%s %.10f\n' % (uttid, p2)
            f3c += '%s %.10f\n' % (uttid, p3)

    f1.write(f1c)
    f2.write(f2c)
    f3.write(f3c)
    f1.close()
    f2.close()
    f3.close()

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s",
    )

    main()
